{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using Spark MLLib\n",
    "\n",
    "This is an example of building up a machine learning pipeline and doing classification using MLlib in Spark.\n",
    "\n",
    "Group Project for Big Data Programming, Fall 2017\n",
    "\n",
    "### Project Contributors:\n",
    "Caleb Hulburt   \n",
    "Mohammad Azim   \n",
    "Yao Jin   \n",
    "Xian Lai   \n",
    "\n",
    "========================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mhow to set up AWS cluster for spark.md\u001b[0m*  \u001b[01;32mTweets_analysis_SparkStreaming.ipynb\u001b[0m*\r\n",
      "\u001b[01;32mREADME.md\u001b[0m*                               \u001b[01;32mTweetsListener.py\u001b[0m*\r\n",
      "\u001b[01;32mSpark_machine_learning_pipeline.ipynb\u001b[0m*   \u001b[01;32mTweetsStreamingPlot.py\u001b[0m*\r\n",
      "\u001b[01;32mTweets_analysis_spark.ipynb\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.ml.feature as f\n",
    "import pyspark.ml.classification as C\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator as BCE\n",
    "import pyspark.ml.tuning as t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Spark\n",
    "set up the configuration and Spark Context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"machine learning pipeline\")\n",
    "sc   = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up SparkSession and use the DataFrameReader to parse and read csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"machine learning pipeline example\")\\\n",
    "    .config(conf=conf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We read in the dataset in csv file with spark session so the data will be distributed onto cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ss.read.csv(\\\n",
    "    path=\"../data/OnlineNewsPopularity.csv\",\\\n",
    "    inferSchema=True,\\\n",
    "    header=True,\\\n",
    "    ignoreLeadingWhiteSpace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop some unpredictive features and sample 1/10 data points out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('url', 'timedelta', 'is_weekend')\n",
    "df = df.sample(False, 0.1)\n",
    "df1 = df.alias(\"df1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features in the dataset are binary and related. So we merge them as one feature and use interger to encode the values. This will reduce the dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeFeatures(dataframe, old_feats, new_feat):\n",
    "    \"\"\" merge binary features in dataframe with int encoding.\n",
    "    \"\"\"\n",
    "    count = 0.0\n",
    "    dataframe = dataframe.withColumn(new_feat, F.lit(count))\n",
    "    for old_feat in old_feats:\n",
    "        dataframe = dataframe.withColumn(new_feat,\n",
    "            F.when(dataframe[old_feat]==1.0,count).\n",
    "            otherwise(dataframe[new_feat]))\n",
    "\n",
    "        dataframe = dataframe.drop(old_feat)\n",
    "        count += 1\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = ['data_channel_is_lifestyle',\n",
    "            'data_channel_is_entertainment',\n",
    "            'data_channel_is_bus',\n",
    "            'data_channel_is_socmed',\n",
    "            'data_channel_is_tech',\n",
    "            'data_channel_is_world',]\n",
    "\n",
    "weekdays = ['weekday_is_monday',\n",
    "            'weekday_is_tuesday',\n",
    "            'weekday_is_wednesday',\n",
    "            'weekday_is_thursday',\n",
    "            'weekday_is_friday',\n",
    "            'weekday_is_saturday',\n",
    "            'weekday_is_sunday',]\n",
    "\n",
    "df1 = mergeFeatures(df1, channels, 'data_channel')\n",
    "df1 = mergeFeatures(df1, weekdays, 'weekday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeOutlier(df_0, n_std=5):\n",
    "    \"\"\"\n",
    "    remove the rows with values more than n std away \n",
    "    from mean.\n",
    "    \"\"\"\n",
    "    avgExpr = []\n",
    "    stdExpr = []\n",
    "    columns = df_0.columns\n",
    "    \n",
    "    for col in columns:\n",
    "        avgExpr.append(F.mean(df_0[col]).alias(col))\n",
    "        stdExpr.append(F.stddev(df_0[col]).alias(col))\n",
    "        \n",
    "    avgs = df_0.agg(*avgExpr).collect()[0]\n",
    "    stds = df_0.agg(*stdExpr).collect()[0]\n",
    "\n",
    "    filterExpr = F.lit(True)\n",
    "    for col in columns:\n",
    "        mask = (F.abs(df_0[col] - avgs[col]) <= n_std * stds[col])\n",
    "        filterExpr = filterExpr & mask\n",
    "\n",
    "    return df_0.filter(filterExpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = df1.alias(\"df2\")\n",
    "df2 = removeOutlier(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binarize the label column and split the training/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn('shares', df2.shares.cast(DoubleType()))\n",
    "train, test = df2.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure an ML pipeline, which consists of 5 stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(train.columns)\n",
    "features.remove('shares')\n",
    "vectorAssembler = f.VectorAssembler(\\\n",
    "    inputCols=features, \n",
    "    outputCol=\"features\")\n",
    "\n",
    "binarizer = f.Binarizer(\\\n",
    "    threshold=1400, \n",
    "    inputCol='shares', \n",
    "    outputCol='label')\n",
    "\n",
    "stringIndexer = f.StringIndexer(\\\n",
    "    inputCol='label',\n",
    "    outputCol = 'indexed')\n",
    "\n",
    "pca = f.PCA(\\\n",
    "    inputCol=\"features\",\\\n",
    "    outputCol=\"pca_features\")\n",
    "\n",
    "lr = C.LogisticRegression(\\\n",
    "    featuresCol=\"pca_features\",\\\n",
    "    labelCol=\"indexed\",\\\n",
    "    standardization=True)\n",
    "\n",
    "pipeline = Pipeline(stages=[\\\n",
    "    vectorAssembler,\\\n",
    "    binarizer,\\\n",
    "    stringIndexer,\\\n",
    "    pca,\\\n",
    "    lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch and CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = t.ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .addGrid(pca.k, [8, 16, 24])\\\n",
    "    .build()\n",
    "    \n",
    "cv = t.CrossValidator(\\\n",
    "    estimator=pipeline,\\\n",
    "    estimatorParamMaps=paramGrid,\\\n",
    "    evaluator=BCE(),\\\n",
    "    numFolds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline fitting and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_49cdbc936d5affd57467"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6207929035684879,\n",
       " 0.6316329348940822,\n",
       " 0.6777289591238491,\n",
       " 0.5627055973000475,\n",
       " 0.5505872227370847,\n",
       " 0.6383518404589555,\n",
       " 0.4972413351392801,\n",
       " 0.511202552167608,\n",
       " 0.5852187965473435,\n",
       " 0.6225375153926213,\n",
       " 0.6416940157243853,\n",
       " 0.6800620713797927,\n",
       " 0.590914550351561,\n",
       " 0.6011479554059808,\n",
       " 0.6403497837318773,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.6221402663219725,\n",
       " 0.632800251663499,\n",
       " 0.6782703431160273,\n",
       " 0.6206429183844815,\n",
       " 0.6307319083278913,\n",
       " 0.6770392265066962,\n",
       " 0.6193327699114692,\n",
       " 0.6285893175997477,\n",
       " 0.6753668183986277,\n",
       " 0.6233543226174348,\n",
       " 0.6424057138396138,\n",
       " 0.68026711987045,\n",
       " 0.6231380864664087,\n",
       " 0.6399883380573146,\n",
       " 0.6776448437132119,\n",
       " 0.6234792992812712,\n",
       " 0.6374529785477373,\n",
       " 0.6763572763188984]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7212447051156727"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BCE()\n",
    "evaluator.evaluate(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
